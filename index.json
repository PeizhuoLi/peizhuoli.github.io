[{"authors":["admin"],"categories":null,"content":"My name is Peizhuo Li (李沛卓). I am a direct doctorate student at Interactive Geometry Lab under the supervision of Prof. Olga Sorkine-Hornung. My research interest lies in the intersection between deep learning and computer graphics. In particular, I am interested in practical problems related to character animation. Prior to my PhD study, I was an intern at Visual Computing and Learning lab at Peking University and advised by Prof. Baoquan Chen.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://peizhuoli.github.io/author/peizhuo-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/peizhuo-li/","section":"authors","summary":"My name is Peizhuo Li (李沛卓). I am a direct doctorate student at Interactive Geometry Lab under the supervision of Prof. Olga Sorkine-Hornung. My research","tags":null,"title":"Peizhuo Li","type":"authors"},{"authors":["Qingqing Zhao","Peizhuo Li","Wang Yifan","Olga Sorkine-Hornung","Gordon Wetzstein"],"categories":null,"content":"","date":1722470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1722470400,"objectID":"98a132f5299c2eb7a32c80b3581a4446","permalink":"https://peizhuoli.github.io/publication/pose-to-motion/","publishdate":"1970-01-01T01:33:44+01:00","relpermalink":"/publication/pose-to-motion/","section":"publication","summary":"We introduce a neural motion synthesis approach that uses accessible pose data to generate plausible character motions by transferring motion from existing motion capture datasets. Our method effectively combines motion features from the source character with pose features of the target character and performs robustly even with small or noisy pose datasets. User studies indicate a preference for our retargeted motions, finding them more lifelike, enjoyable to watch, and exhibiting fewer artifacts.","tags":null,"title":"Pose-to-Motion: Cross-Domain Motion Retargeting with Pose Prior","type":"publication"},{"authors":["Peizhuo Li","Sebastian Starke","Yuting Ye","Olga Sorkine-Hornung"],"categories":null,"content":"","date":1719792000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719792000,"objectID":"5225bc03de27b36b6b86c4f978a99df9","permalink":"https://peizhuoli.github.io/publication/walkthedog/","publishdate":"1970-01-01T01:33:44+01:00","relpermalink":"/publication/walkthedog/","section":"publication","summary":"We introduce a novel approach to learn a common phase manifold from motion datasets across different characters, such as human and dog, using vector quantized periodic autoencoders. This manifold clusters semantically similar motions into the same connected component and aligns them temporally without supervision. Our method enables effective motion matching and supports applications in motion retrieval, transfer, and stylization.","tags":null,"title":"WalkTheDog: Cross-Morphology Motion Alignment via Phase Manifolds","type":"publication"},{"authors":["Peizhuo Li","Tuanfeng Y. Wang","Timur Levent Kesdogan","Duygu Ceylan","Olga Sorkine-Hornung"],"categories":null,"content":"","date":1711929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711929600,"objectID":"6ecf1a59f89bba8015b7f62b4bc9dc36","permalink":"https://peizhuoli.github.io/publication/manifold-aware-transformers/","publishdate":"1970-01-01T01:33:44+01:00","relpermalink":"/publication/manifold-aware-transformers/","section":"publication","summary":"Data driven and learning based solutions for modeling dynamic garments have significantly advanced, especially in the context of digital humans. We model the dynamics of a garment by exploiting its local interactions with the underlying human body. At the core of our approach is a mesh-agnostic garment representation and a manifold-aware transformer network design, which together enable our method to generalize to unseen garment and body geometries.","tags":null,"title":"Neural Garment Dynamics via Manifold-Aware Transformers","type":"publication"},{"authors":["Weiyu Li*","Xuelin Chen*","Peizhuo Li","Olga Sorkine-Hornung","Baoquan Chen"],"categories":null,"content":"","date":1690848000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690848000,"objectID":"47ed83fcdeb151fc079638b7f1b0fa8f","permalink":"https://peizhuoli.github.io/publication/genmm/","publishdate":"1970-01-01T01:33:43+01:00","relpermalink":"/publication/genmm/","section":"publication","summary":"We present Generative Motion Matching (GenMM), a generative model that \"mines\" as many diverse motions as possible from a single or few example sequences. GenMM is training-free and can synthesize a high-quality motion within a fraction of a second, even with highly complex and large skeletal structures.","tags":null,"title":"Example-based Motion Synthesis via Generative Motion Matching","type":"publication"},{"authors":["Sigal Raab","Inbal Leibovitch","Peizhuo Li","Kfir Aberman","Olga Sorkine-Hornung","Daniel Cohen-Or"],"categories":null,"content":"","date":1685577600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685577600,"objectID":"b967c89cb25aabddc0f0b87d31895854","permalink":"https://peizhuoli.github.io/publication/modi/","publishdate":"1970-01-01T01:33:43+01:00","relpermalink":"/publication/modi/","section":"publication","summary":"The emergence of neural networks revolutionized motion synthesis, yet synthesizing diverse motions remains challenging. We present MoDi, an unsupervised generative model trained on a diverse, unstructured, unlabeled dataset, capable of synthesizing high-quality, diverse motions. Despite dataset's lack of structure, MoDi yields a structured latent space for semantic clustering, enabling applications like semantic editing and crowd simulation. We also introduce an encoder that inverts real motions into MoDi's motion manifold, addressing ill-posed challenges like completion from prefix and spatial editing, achieving state-of-the-art results surpassing recent techniques.","tags":null,"title":"MoDi: Unconditional Motion Synthesis from Diverse Data","type":"publication"},{"authors":["Peizhuo Li","Kfir Aberman","Zihan Zhang","Rana Hanocka","Olga Sorkine-Hornung"],"categories":null,"content":"","date":1659312000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659312000,"objectID":"795dad20da360b39e1ae58e91d14ebd4","permalink":"https://peizhuoli.github.io/publication/ganimator/","publishdate":"1970-01-01T01:33:42+01:00","relpermalink":"/publication/ganimator/","section":"publication","summary":"We present GANimator, a generative model that learns to synthesize novel motions from a single, short motion sequence. GANimator generates motions that resemble the core elements of the original motion, while simultaneously synthesizing novel and diverse movements. It also enables applications including crowd simulation, key-frame editing, style transfer, and interactive control for a variety of skeletal structures e.g., bipeds, quadropeds, hexapeds, and more, all from a single input sequence.","tags":null,"title":"GANimator: Neural Motion Synthesis from a Single Sequence","type":"publication"},{"authors":["Peizhuo Li","Kfir Aberman","Rana Hanocka","Libin Liu","Olga Sorkine-Hornung","Baoquan Chen"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"feb669f16066b57422922ebc97944d11","permalink":"https://peizhuoli.github.io/publication/neural-blend-shapes/","publishdate":"1970-01-01T01:33:41+01:00","relpermalink":"/publication/neural-blend-shapes/","section":"publication","summary":"We develop a neural technique for articulating 3D characters using enveloping with a pre-defined skeletal structure, which is essential for animating character with motion capture (mocap) data. Furthermore, we propose neural blend shapes -- a set of corrective pose-dependent shapes that is used to address the notorious artifacts caused by standard rigging and skinning technique in joint region.","tags":null,"title":"Learning Skeletal Articulations with Neural Blend Shapes","type":"publication"},{"authors":["Kfir Aberman*","Peizhuo Li*","Dani Lischinski","Olga Sorkine-Hornung","Daniel Cohen-Or","Baoquan Chen"],"categories":null,"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"11d7d92fd6b3c136219fb16a8141ae6b","permalink":"https://peizhuoli.github.io/publication/skeleton-aware/","publishdate":"1970-01-01T01:33:40+01:00","relpermalink":"/publication/skeleton-aware/","section":"publication","summary":"We introduce a novel deep learning framework for data-driven motion retargeting between skeletons, which may have different structure, yet corresponding to homeomorphic graphs. Importantly, our approach learns how to retarget without requiring any explicit pairing between the motions in the training set.","tags":null,"title":"Skeleton-Aware Networks for Deep Motion Retargeting","type":"publication"}]